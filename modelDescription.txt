Logistic regression is a type of Generalized Linear Model (GLM) used to model a binary response variable, such as “yes”/“no”,  “male”/“female”, “success”/“failure”, or 0/1.  Predictor variables can be categorical or continuous.  The parameters of the model are linear, and multiplying parameters by variables equals the log-odds (logit) of “success”:
$log(\frac{\theta}{1-\theta}) = \beta_0 + \beta_1X_1 + … + \beta_pX_p$, where $\theta = P(success)$ and $p = number of predictors$.  Probability of success can be calculated using the ilogit function:  $\theta = \frac{e^( \beta_0 + \beta_1X_1 + … + \beta_pX_p)}{1 + e^( \beta_0 + \beta_1X_1 + … + \beta_pX_p)}$.  For classification purposes, a cutoff for $p$ can chosen (e.g. 0.5), so that $p>cutoff$ will be classified as a success, and $p<cutoff$ will be classified as a failure.  Variations of logistic regression can also be applied to response variables with more than 2 levels.

Logistic regression has the benefit of relatively straightforward interpretation and efficient computation, but has the drawback of requiring certain assumptions regarding the variance and distribution of errors in the model.

Classification trees make predictions by dividing the predictor space into a number of regions, and determining which region the predictor values of the new observation fall into by applying a series of splits, each based on the value of a single predictor.  The response for the new observation is then predicted to be the predominant class observed in the region.  First a large tree is grown, with the goal of maximizing prediction accuracy, resulting in a tree with many regions, each containing a small number of observations.  But this complex tree will generally be overfit, with low bias and high variance, so it gets pruned back to an optimal size, determined by cross validation, that will have higher bias but lower variance, and ideally perform better when predicting on new data.

Classification trees have several benefits.  Their output is easy to interpret, and predictors don’t need to be scaled.  Also, they don’t require any statistical assumptions, and they inherently select variables and account for interactions between variables.  Drawbacks include the need to prune the tree, the fact that small changes in data can drastically change output, and their greedy algorithm (splits are made one a time, without considering potential splits further down the tree).

"Random forest models are an improvement on bagged tree models, which improve on basic decision trees by using the bootstrap to take many samples from the training data set and producing an unpruned tree from each sample, then averaging the predictions of those trees to get the bagged tree model.  The averaging of hundreds of high-variance trees results in a much lower variance model.  The random forest model is a further improvement on the bagged tree model.  It works by decorrelating the trees that are generated and averaged together.  In a bagged tree model, many of the trees can end up being similar, with the main splits dominated by the strongest predictor(s).  The correlation between these trees means that averaging them results in a smaller reduction in variance than desired.  To remedy this, random forest models consider only a random subset of predictors for each split, resulting in less correlation between trees, and lower variance in the final model.  The number of predictors considered for each split is a tuning parameter, whose value can be chosen using cross validation.

Random forest models share some benefits with classification trees--they require no statistical assumptions, and they automatically select variables and account for interactions between variables.  However, they improve on the predictive ability of classification trees, but at the expense of interpretability."  